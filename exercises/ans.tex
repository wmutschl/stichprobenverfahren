\begin{Solution}{{Aufgabe 1:}}
		\begin{enumerate}
		\item Die Anzahl an Stichproben mit Element $k$ ist $\binom{N-1}{n-1}$. Hinzufügen von Element $k$ zu diesen Stichproben ergibt Stichprobengröße $n$. $k$ wird aber auch zur Grundgesamtheit hinzugefügt, diese hat dann $N$ Elemente. Also laut LaPlace Definition von Wahrscheinlichkeiten gilt für die Einschlusswahrscheinlichkeit erster Ordnung:
		$$\pi_k = \frac{\binom{N-1}{n-1}}{\binom{N}{n}} = \frac{\frac{(N-1)!}{(n-1)!(N-1-n+1)!}}{\frac{N!}{n!(N-n)!}}=\frac{\frac{(N-1)!}{(n-1)!}}{\frac{N!}{n!}}=\frac{n}{N}$$
		Für die Einschlusswahrscheinlichkeiten zweiter Ordnung gilt analog:
		$$\pi_{kl} = \frac{\binom{N-2}{n-2}}{\binom{N}{n}} = \frac{\frac{n!}{(n-2)!}}{\frac{N!}{(N-2)!}} = \frac{n(n-1)}{N(N-1)}$$
		\item Es gilt: $\pi_k = \pi_l = \frac{n}{N}$ und $\pi_{kl} = \frac{n(n-1)}{N(N-1)}$. Dann folgt für die Kovarianz:
		\begin{align*}
		Cov(I_k,I_l) &= \pi_{kl} - \pi_k \pi_l = \frac{n(n-1)}{N(N-1)} - \frac{n}{N}\frac{n}{N}\\
		&= -\frac{n}{N}\left(\frac{1-n}{N-1}+\frac{n}{N}\frac{N-1}{N-1}\right) = \frac{-n}{N}\left(\frac{1-n+n/N(N-1)}{N-1}\right) \\
		&= \frac{-n}{N}\left(\frac{1-n/N}{N-1}\right) <0
		\end{align*}
		da $n/N>0$, $1-n/N>0$ und $N-1 >0$.
	\end{enumerate}
    Der R-Code könnte folgendermaßen aussehen:
	\begin{lstlisting}
	## Aufgabe Einschlusswahrscheinlichkeiten
	# Matrix mit Einschlusswahrscheinlichkeiten
	Iks <- function(x,y) as.numeric(is.element(x,y))
	N <- 4
	n <- 2

	# a)
	S <- combn(1:N,n)
	M <- choose(N,n)
	ps <- rep(1/M,M)
	ind <- apply(S,2,function(z) Iks(1:N,z)); ind
	pi_k = colSums(t(ind)*ps);
	round(pi_k,2)
	sum(pi_k)

	#c)
	M <- 3
	S <- cbind(c(1,3),c(1,4),c(2,4))
	ps <- c(0.1,0.6,0.3)
	ind <- apply(S,2,function(z) Iks(1:N,z)); ind
	pi_k <- colSums(t(ind)*ps)
	round(pi_k,2)
	sum(pi_k)

	#d)
	pi_kl <- matrix(NA,N,N)
	for (k in 1:N){
		for (l in 1:N) {
			pi_kl[k,l] <- sum(apply(S,2,function(z) Iks(k,z)*Iks(l,z))*ps)
		}
	}
	Delta_kl <- matrix(NA,N,N)
	for (k in 1:N){
		for (l in 1:N) {
			Delta_kl[k,l] <- pi_kl[k,l] - pi_kl[k,k]*pi_kl[l,l]
		}
	}
	\end{lstlisting}

\end{Solution}
\begin{Solution}{{Aufgabe 2:}}
Der R-Code könnte folgendermaßen aussehen:
\begin{lstlisting}
##################################################################
#### Aufgabe Schaetzung mithilfe von Einschlusswahrscheinlichkeiten
##################################################################
# Matrix mit Einschlusswahrscheinlichkeiten
Iks <- function(x,y) as.numeric(is.element(x,y))
Y <- c(1,2,5,12,30)
N <- length(Y)
n <- 3
ps <- 1:M/sum(1:M); round(ps,3)
#a)
M <- choose(N,n);M
#b)
S <- combn(N,n);S
ind <- apply(S,2,function(z) Iks(1:N,z)); ind
#c)
pi_k <- colSums(t(ind)*ps);round(pi_k,3)
pi_kl <- matrix(NA,N,N)
for (k in 1:N){
	for (l in 1:N){
		pi_kl[k,l] <- sum(apply(S,2,function(z) Iks(k,z)*Iks(l,z))*ps)
	}
}
round(pi_kl,3)
#d)
Delta_kl <- matrix(NA,N,N)
for (k in 1:N){
	for (l in 1:N) {
		Delta_kl[k,l] <- pi_kl[k,l] - pi_kl[k,k]*pi_kl[l,l]
	}
}
round(Delta_kl,2)
#e)
ybar.hat <- 1/N*apply(S,2,function(z) sum(Y[z]/pi_k[z]))
round(ybar.hat,2)
#f)
mean(Y) #wahrer Wert
sum(ybar.hat*ps) #unverzerrter Schaetzer ergibt wahren Wert
#g)
# Funktion die die Varianz des Horvitz-Thompson Schaetzers fuer jede Stichprobe schaetzt
vhatHT <- function(s){
	n <- length(s)
	sl <- rep(NA,n)
	sk <- sl
	for (j1 in 1:n){
		k <- s[j1]
		for (j2 in 1:n) {
			l <- s[j2]
			sl[j2] <- 1/pi_kl[k,l]*(pi_kl[k,l]/(pi_k[k]*pi_k[l])-1)*Y[k]*Y[l]
		}
		sk[j1] <- sum(sl)
	}
	sum(sk)/N^2
}
vHT <- apply(S,2,vhatHT)
round(vHT,2)
# Erster Wert ist negativ! Dies kann passieren beim Varianz Schaetzer von Horvitz-Thompson

#h) Alternativ Yates-Grundi Schaetzer
vhatYG <- function(s){
	n <- length(s)
	sl <- rep(NA,n)
	sk <- sl
	for (j1 in 1:n){
		k <- s[j1]
		for (j2 in 1:n) {
			l <- s[j2]
			sl[j2] <- Delta_kl[k,l]/pi_kl[k,l]*(Y[k]/pi_k[k]-Y[l]/pi_k[l])^2
		}
		sk[j1] <- sum(sl)
	}
	sum(sk)*(-1)/(2*N^2)
}
vYG <- apply(S,2,vhatYG)
round(vYG,2)

#i)
sum((ybar.hat-mean(Y))^2*ps) # wahrer Wert der Varianz des Schaetzers
sum(vHT*ps)
sum(vYG*ps)
\end{lstlisting}
\end{Solution}
\begin{Solution}{{Aufgabe 3:}}
	\begin{enumerate}
\item Der $\pi$ Schätzer für die Merkmalssumme vereinfacht sich zu:
\begin{align*}
\hat{t}_\pi = \sum_U I_k \frac{y_k}{\pi_k} = \sum_U I_k \frac{y_k}{n/N} = \frac{N}{n} \sum_U I_k y_k =  \frac{N}{n} \sum_s y_k = N \bar{y}_s
\end{align*}
Dieser ist unverzerrt, da
\begin{align*}
E\left(\frac{N}{n} \sum_s y_k\right) = \frac{N}{n} E\left(\sum_U I_k y_k\right) = \frac{N}{n} \sum_U y_k E(I_k) = \frac{N}{n} \sum_U y_k \frac{n}{N} = \sum_U y_k
\end{align*}
\item Die Varianz lässt sich umformen zu:
\begin{align*}
V(\hat{t}_\pi) &= \sum\sum_U (\pi_{kl}-\pi_k\pi_l)\frac{y_k}{\pi_k}\frac{y_l}{\pi_l}= \sum_U \pi_k(1-\pi_k)\left(\frac{y_k}{\pi_k}\right)^2 + \sum\sum_{U,k\neq l} (\pi_{kl}-\pi_k\pi_l)\frac{y_k}{\pi_k}\frac{y_l}{\pi_l}\\
&= \sum_U \frac{n}{N}\left(1-\frac{n}{N}\right)\left(\frac{y_k}{n/N}\right)^2 + \sum\sum_{U,k\neq l}\left(\frac{n}{N}\frac{n-1}{N-1}-\frac{n}{N}\frac{n}{N}\right)\frac{y_k}{n/N}\frac{y_l}{n/N}\\
&= N^2 \frac{n}{N} \left(1-\frac{n}{N}\right)\frac{1}{n^2}\sum_U y_k^2 + N^2\left(\frac{n}{N}\frac{n-1}{N-1}-\frac{n}{N}\frac{n}{N}\right)\frac{1}{n^2} \sum\sum_{U,k\neq l} y_k y_l\\
&= N^2\frac{1}{n}\left(\frac{1}{N}-\frac{n}{N^2}\right)\sum_U y_k^2 + N^2\frac{1}{n} \left(\frac{(n-1)}{N(N-1)}-\frac{n}{N^2}\right)\sum\sum_{U,k\neq l}y_k y_l\\
&= N^2\frac{1}{n}\left(\frac{N-n}{N^2}\right)\sum_U y_k^2 + N^2 \frac{1}{n}\left(\frac{N^2(n-1)-nN(N-1)}{N^2N(N-1)}\right)\sum\sum_{U,k\neq l}y_ky_l\\
&= N^2\frac{1}{n}\left(\frac{N-n}{N^2}\right)\sum_U y_k^2 + N^2 \frac{1}{n}\left(\frac{nN^2-N^2-nN^2+nN}{N^2N(N-1)}\right)\sum\sum_{U,k\neq l}y_ky_l\\
&= N^2\frac{1}{n}\left(\frac{N-n}{N-1}\frac{N-1}{N^2}\right)\sum_U y_k^2 + N^2 \frac{1}{n}\left(\frac{-(N-n)}{N^2(N-1)}\right)\sum\sum_{U,k\neq l}y_ky_l\\
&= N^2\frac{1}{n}\frac{N-n}{N-1}\left(\left(\frac{1}{N}-\frac{1}{N^2}\right)\sum_U y_k^2 - \frac{1}{N^2}\sum\sum_{U,k\neq l}y_k y_l\right)\\
&= N^2\frac{1}{n}\frac{N-n}{N-1}\left(\frac{1}{N}\sum_U y_k^2 - \frac{1}{N^2} \sum_U y_k^2 - \frac{1}{N^2}\sum\sum_{U,k\neq l}y_k y_l\right)\\
&= N^2\frac{1}{n}\frac{N-n}{N-1}\left(\frac{1}{N}\sum_U y_k^2 - \frac{1}{N^2}\sum\sum_{U}y_k y_l\right)\\
&=N^2\frac{1-f}{n} S_{y_U}^2
\end{align*}
\item Der Varianzschätzer ist unverzerrt, da:
\begin{align*}
E\left(\hat{V}(\hat{t}_\pi)\right) &= E\left(\sum\sum_s \check{\Delta}\check{y_k}\check{y_l}\right) = E\left(\sum\sum_U I_k I_l\check{\Delta}\check{y_k}\check{y_l}\right) = \sum\sum_U E(I_k I_l)\check{\Delta}\check{y_k}\check{y_l} \\
&= \sum\sum_U \pi_{kl} \frac{\Delta_{kl}}{\pi_{kl}} \check{y_k}\check{y_l} = \sum\sum_U \Delta_{kl}\check{y_k}\check{y_l} = V(\hat{t}_\pi)
\end{align*}
\end{enumerate}
\end{Solution}
\begin{Solution}{{Aufgabe 4:}}
\begin{enumerate}
	\item Für die Einschlusswahrscheinlichkeit gilt $Pr(\varepsilon_k<\pi_B) = \pi_k = \pi_B$. Für $k \neq l$ gilt, dass das Ereignis \enquote{$k$ und $l$ werden beide ausgewählt} unabhängig ist, also $I_k$ und $I_l$ unabhängig und identisch verteilt sind. Somit ist der Einschlussindikator $I_k$ Bernoulli verteilt mit Parameter $\pi_B$. Es gilt: $E(I_k)=\pi_B$, $V(I_k)=\pi_B(1-\pi_B) = \Delta_{kk}$ und für $k \neq l$: $Cov(I_k,I_l)=\pi_B^2-\pi_B\pi_B = 0 = \Delta_{kl}$. Die Stichprobengröße ist zufällig und Binomial verteilt mit Parametern $N$ und $\pi_B$, mit $E(n_s)=N\pi_B$ und $V(n_s)=N\pi_B(1-\pi_B)$. Somit ist das Stichprobendesign gegeben durch: $$p(s)=\underbrace{\pi_B \cdot ... \cdot \pi_B}_{n_s}\cdot \underbrace{(1-\pi_B) \cdot ... \cdot (1-\pi_B)}_{N-n_s} = \pi_B^{n_s}(1-\pi_B)^{N-n_s}$$
	\item $Pr(n_s = n) = \binom{N}{n}\pi_B^n(1-\pi_B)^{N-n}$
	\item $\hat{t}_\pi = \frac{1}{\pi_B}\sum_s y_k$ mit Varianz $V_{BE}(\hat{t}_\pi)= \frac{1-\pi_B}{\pi_B} \sum_U y_k^2$
	\item $\sum_U y_k^2$ lässt sich umformen zu: $\sum_U y_k^2 = (N-1)S_{Y_U}^2 + N(\bar{y}_U)^2 = \left[1-\frac{1}{N}+\frac{1}{(cv_{y_U})^2}\right]N S_{y_U}^2$. Um einen fairen Vergleich zu gewährleisten, setzen wir $E(n_s)=N\pi=n$, dann ist der Designeffekt gegeben durch:
	\begin{align*}
	deff = \frac{V_{BE}(\hat{t}_\pi)}{V_{SI}(\hat{t}_\pi)} = 1-\frac{1}{N}+\frac{1}{(cv_{y_U})^2}.
	\end{align*}
	Oft liegt der Variationskoeffizient zwischen $0.5 \leq cv_{y_U} \leq 1$, was einem Designeffekt von ungefähr 2 bis 5 entsprechen würde. Somit lässt sich zusammenfassen, dass das sogenannte Bernoulli Sampling (BE) oft weniger präzise für den $\pi$-Schätzer ist als die einfache Zufallsstichprobe ohne Zurücklegen (SI). Der Grund liegt in der zusätzlichen Variabilität in der Stichprobengröße. Dies kann man berücksichtigen und beispielsweise einen anderen unverzerrten Schätzer verwenden, z.B. $\hat{t}_{alt}=\frac{n}{n_s} \hat{t}_\pi$.
\end{enumerate}
\end{Solution}
\begin{Solution}{{Aufgabe 5:}}
Es gibt $M=\binom{N}{n} = \binom{5}{3} = 10$ mögliche Stichproben. In der Grundgesamtheit ist das Mittel gleich 2 und der Median gleich 1.
\begin{center}
\begin{tabular}{|c|c|c|c|}
	\hline
	\multicolumn{2}{|c|}{Stichprobe} & Mittelwert & Median \\
	\hline
	1 & 1 2 3 & 4/3 & 1 \\
	\hline
	2 & 1 2 4 & 5/3 & 1 \\
	\hline
	3 & 1 2 5 & 9/3 & 3 \\
	\hline
	4 & 1 3 4 & 4/3 & 1 \\
	\hline
	5 & 1 3 5 & 8/3 & 3 \\
	\hline
	6 & 1 4 5 & 9/3 & 3 \\
	\hline
	7 & 2 3 4 & 2/3 & 1 \\
	\hline
	8 & 2 3 5 & 6/3 & 1 \\
	\hline
	9 & 2 4 5 & 7/3 & 1 \\
	\hline
	10 & 3 4 5 & 6/3 & 1 \\
	\hline
	$\sum$ &  & 20 & 16 \\
	\hline
\end{tabular}
\end{center}
Das Stichprobenmittel (20/10) ist erwartungstreu für den Merkmalsdurchschnitt, aber Stichprobenmedian ist nicht erwartungstreu für den Median der Grundgesamtheit.
\end{Solution}
\begin{Solution}{{Aufgabe 6:}}
Es handelt sich um keine einfache Zufallsstichprobe vom Umfang $n=1$, denn: $p_1=\frac{500}{1000}=$, $p_2=p_3=0.2$ und $p_4=0.1$, d.h. die Auswahlwahrscheinlichkeiten sind verschieden.\\
Das Auswahlverfahren ist eine einfache Zufallsauswahl genau dann, wenn die Umfänge der Lieferungen alle gleich groß sind.
\end{Solution}
\begin{Solution}{{Aufgabe 7:}}
	$N=676, n=50$. Es gilt: $\hat{t}_\pi = N \bar{y}_s = N\frac{1}{n} \sum_s f_k y_k = \frac{1471}{50} = 676 \cdot 29.42 = 19887.92$.
	$V(\hat{t}_\pi) = N^2 \frac{1-n/N}{n} S_{y_s}^2= 676^2 \frac{1-50/676}{50} \frac{1}{49}(54497-50 \cdot 29.42^2) = 1937990$. 80\% Konfidenzintervall: $\hat{t}_\pi \pm 1.28 \cdot \sqrt{1937990} = [18103.84;21672]$.
\end{Solution}
\begin{Solution}{{Aufgabe 8:}}
Der R-Code könnte folgendermaßen aussehen:
\begin{lstlisting}
psid <- read.csv2("psid.csv")
N <- nrow(psid)
n <- 20
Y <- psid$wage
f <- n/N
Ybar <- mean(Y);Ybar
V.Ybar <- (1-f)/n*var(Y); V.Ybar

f_var <- function(y,N){
	n <- length(y)
	f <- n/N
	return((1-f)/n*var(y))
}

B <- 100000
m <- rep(NA,B)
v <- rep(NA,B)
for (b in 1:B) {
	y <- sample(Y,n)
	m[b] <- mean(y)
	v[b] <- f_var(y,N)
}

vm <- v/10^6
V.Ybarm <- V.Ybar/10^6

plot(density(m),lwd=2,xlab='Schaetzwert',main='')
arrows(Ybar,5e-06,Ybar,0,length=0.1,angle=25)
text(Ybar,7.5e-06,expression(bar(y)[U]))

plot(density(vm),lwd=2,xlab='Schaetzwert in Millionen',main='')
arrows(V.Ybarm+1000,0.002,V.Ybarm,0,length=0.1,angle=25)
text(V.Ybarm+1000,0.0026,expression(V(bar(y)[U])))
\end{lstlisting}
Bei der Normalverteilung ist das Konfidenzintervall im Allgemeinen $\hat{\theta} \pm u_{1-\alpha/2}[V(\hat{\theta})]^{1/2}$. Wir treffen damit implizit Aussagen über Asymptotische Eigenschaften des Schätzers auf Grundlage eines Zentralen Grenzwertsatzes. Auch für abhängig identisch verteilte Zufallsvariablen (wie beim Ziehen ohne Zurücklegen) gibt es solch einen Satz, allerdings gilt, dass falls die Verteilung von $Y$ stark schief ist (wie im Beispiel der Fall), dann benötigen wir einen sehr hohen Stichprobenumfang für die Konvergenz zur Normalverteilung. In diesem Fall ist es folglich sinnvoller sich die Konfidenzintervalle zu simulieren/bootstrapen.
\end{Solution}
\begin{Solution}{{Aufgabe 9:}}
	\begin{enumerate}
\item Der $\pi$-Schätzer in einfachen Zufallsstichproben für einen Anteil ist gerade der $\pi$-Schätzer für den Durchschnitt. Also: $\hat{P}=\hat{\bar{y}}_\pi = \frac{1}{n}\sum_s y_k$. Die Varianz ist gegeben durch:
\begin{align*}
V(\hat{P}) &= \frac{1-f}{n} S_{y_U}^2 = \frac{N-n}{Nn}\frac{1}{N-1}\left(\sum_U y_k^2-\bar{y}\right) = \frac{1}{n}\frac{N-n}{N-1}\left(\frac{1}{N}\sum_U y_k^2 - \left(\frac{1}{N}\sum_U y_k\right)^2\right)\\
&=\frac{1}{n}\frac{N-n}{N-1} \left(\frac{1}{N}\sum_U y_k\right)\left(1-\sum_U y_k\right) = \frac{1}{n}\frac{N-n}{N-1} P(1-P)
\end{align*}
Dies kann unverzerrt geschätzt werden mit
\begin{align*}
\hat{V}(\hat{P}) &= \frac{1-f}{n}S_{y_s}^2 = \frac{1-f}{n}\frac{n}{n-1}\left(\frac{1}{n}\sum_s y_k\right)\left(1-\frac{1}{n}\sum_s y_k\right) = \frac{1-f}{n-1}\hat{P}(1-\hat{P})
\end{align*}
\item Der R-Code könnte folgendermaßen aussehen:
\begin{lstlisting}
psid <- read.csv2('psid.csv')
N <- nrow(psid)
B <- 10000
n <- 30
f <- n/N
e <- rep(NA,B)
v <- rep(NA,B)
Y <- psid$sector==7
for (i in 1:B){
	y <- sample(Y,n)
	e[i] <- mean(y)
	v[i] <- (1-f)/n*var(y)
}
plot(density(e))
# grob normal verteilt, insbesondere bei hoeheren n
plot(density(v))
# linksschief, definitiv nicht normalverteilt
\end{lstlisting}
\end{enumerate}
\end{Solution}
\begin{Solution}{{Aufgabe 10:}}
Sei $\hat{P}=\hat{\bar{y}}_\pi=\frac{1}{n}\sum_s y_k$ der erwartungstreue Schätzer für den Anteil mit Varianz $V(\hat{P}) = \frac{1}{n} \frac{N-n}{n-1}P(1-P) $. Wir können den Korrekturfaktor vernachlässigen, da $N\approx 80$ Millionen, also  $V(\hat{P}) \approx \frac{1}{n}P(1-P)$. Dann gilt für das Konfidenzintervall (unter Annahme der Normalverteilung): $P \pm \sqrt{V(\hat{P})} u_{1-\alpha/2}$. Länge des Konfidenzintervalls ist $2 \sqrt{V(\hat{P})}$, dies soll gleich $0.1 P$ sein, also:
\begin{align*}
2 \sqrt{V(\hat{P})} u_{1-\alpha/2} &= 2 \sqrt{\frac{P(1-P)}{n}}u_{1-\alpha/2}=0.1P\\
\Leftrightarrow n &= \frac{P(1-P)u_{1-\alpha/2}^2}{0.05^2 P^2}
\end{align*}
Mit $\alpha=0.05$ gilt dann
\begin{enumerate}
	\item $P=0.5 \Rightarrow n \approx 1537$
	\item $P=0.175 \Rightarrow n \approx 7244$
	\item $P=0.05 \Rightarrow n \approx 29196$
\end{enumerate}
\end{Solution}
\begin{Solution}{{Aufgabe 11:}}
\begin{enumerate}
\item Laut Vorlesung gilt für $N=an$:
\begin{align*}
V(\hat{t}_\pi) &= N\cdot SSB = N(SST-SSW) =\frac{N(N-a)}{N-1}SST \left(\frac{N-1}{N-a}-1 +1 -\frac{N-1}{N-a}\frac{SSW}{SST}\right)\\
&= N(N -a)\frac{SST}{N-1}\left(\frac{N-1 -N +a}{N-a} +\delta\right)\\
& \overset{N=an}{=} Na(n-1) S_{y_U}^2 \left(\frac{a-1}{a(n-1)}+\delta\right)\\
& = \frac{N^2}{n} S_{y_U}^2 \left(1-\frac{1}{a}+(n-1)\delta\right)
\end{align*}
Je homogener die Elemente in einer systematischen Stichprobe sind, desto weniger effizient ist der Schätzer.
\item $V_{SI} = N^2\frac{1-f}{n}S_{y_U}^2$ und $V_{SY} = \frac{N^2 S_{y_U}^2}{n} [(1-f)+(n-1)\delta] = N^2\frac{1-f}{n}S_{y_U}^2 + \frac{N^2 S_{y_U}^2}{n}(n-1)\delta$. Der Designeffekt ist dann:
\begin{align*}
deff = \frac{V_{SY}}{V_{SI}} = 1 + \frac{n-1}{1-f}\delta
\end{align*}
Systematisches Sampling ist effizienter als die einfache Zufallsstichprobe falls $\delta <0$.
\item In der Praxis müssen wir versuchen (falls möglich) die Grundgesamtheit so anzuordnen, dass die Merkmalswerte $y_k$ innerhalb der systematischen Stichproben so heterogen wie möglich sind (z.B. durch Nachbarschaften, linearen Trend,...)
\end{enumerate}
\end{Solution}
\begin{Solution}{{Aufgabe 12:}}
Der R Code könnte folgendermaßen aussehen:
	\begin{lstlisting}
	reisanbau <- read.csv2('reisanbau.csv')
	N <- 892
	n <- 25
	X.dot <- 568565
	sum(reisanbau$Reisflaeche/reisanbau$Flaeche)
	y.sum <- X.dot * sum(reisanbau$Reisflaeche/reisanbau$Flaeche)/n
	y.sum
	var(reisanbau$Reisflaeche/reisanbau$Flaeche)
	var.y.sum <- X.dot^2 * var(reisanbau$Reisflaeche/reisanbau$Flaeche)/n
	var.y.sum
	sqrt(var.y.sum)

	lower <- y.sum - sqrt(var.y.sum)*qnorm(0.975)
	upper <- y.sum + sqrt(var.y.sum)*qnorm(0.975)
	cbind(lower, upper)
	\end{lstlisting}
\end{Solution}
\begin{Solution}{{Aufgabe 13:}}
Der R-Code könnte folgendermaßen aussehen:
\begin{lstlisting}
library(pps)
B <- 10000; N <- 5; n <- 3
e_sample <- matrix(NA,B,n)
e_sampford <- matrix(NA,B,n)
p <- 4:8/sum(4:8);p

for (i in 1:B){
	e_sample[i,] <- sample (1:N,n,prob=p)
	e_sampford[i,] <- sampford(p,n)
}
pi_emp_sample <- rep(NA,N)
pi_emp_sampford <- rep(NA,N)

for (i in 1:N){
	pi_emp_sample[i] <- sum(apply(e_sample,1, function(z) i%in%z))
	pi_emp_sampford[i] <- sum(apply(e_sampford,1, function(z) i%in%z))
}

rbind(p*n,round(pi_emp_sample/B,3),round(pi_emp_sampford/B,3))
\end{lstlisting}
\end{Solution}
\begin{Solution}{{Aufgabe 14:}}
Der R-Code könnte folgendermaßen aussehen:
\begin{lstlisting}
library(samplingbook)
data(influenza)
summary(influenza)

# 1) pps.sampling
pps <- pps.sampling(z=influenza$population,n=20,method='sampford')
pps
sample <- influenza[pps$sample,]
sample

# 2) htestimate
pps <- pps.sampling(z=influenza$population,n=20,method='midzuno')
sample <- influenza[pps$sample,]
N <- nrow(influenza)

# Exakte Varianzberechnung
PI <- pps$PI
htestimate(sample$cases, N=N, PI=PI, method='ht')
htestimate(sample$cases, N=N, PI=PI, method='yg')
# Approximierte Varianzschaetzung
pk <- pps$pik[pps$sample]
htestimate(sample$cases, N=N, pk=pk, method='hh')
pik <- pps$pik

# Konfidenzintervale basierend auf der Normalverteilung
est.ht <- htestimate(sample$cases, N=N, PI=PI, method='ht')
est.ht$mean*N
lower <- est.ht$mean*N - qnorm(0.975)*N*est.ht$se
upper <- est.ht$mean*N + qnorm(0.975)*N*est.ht$se
c(lower,upper)
# Wahrer Wert an Grippeerkrankungen
sum(influenza$cases)
\end{lstlisting}

\end{Solution}
\begin{Solution}{{Aufgabe 15:}}
\begin{enumerate}
	\item \begin{align*}
	\pi_k &=Pr(u_k \text{ in } 1) + Pr(u_k \text{ in 2 und nicht in }1) \\
	&= p_k +Pr(u_k \text{ nicht in 1})\cdot Pr(u_k \text{ in 2}|u_k \text{ nicht in 1})\\
	&=p_k + \sum_{l=1,l \neq k}^N Pr(u_l, l\neq k, \text{ in 1}) \cdot Pr(u_k \text{ in 2}|u_l, l \neq k \text{ in 1})\\
	&=p_k + \sum_{l=1,l\neq k}^N\left(\frac{x_l}{\sum_{j=1}^N x_j} - \frac{x_k}{\sum_{j=1}^N x_j-x_l}\right)\\
	&=p_k +\sum_{l=1,l\neq k}^N\left(p_l \cdot \frac{\sum_{j=1}^N x_j}{\sum_{j=1}^N x_j} \cdot \frac{x_k}{\sum_{j=1}^N x_j-x_l}\right)\\
	&=p_k +\sum_{l=1,l\neq k}^N \left(p_l \cdot p_k\cdot  \frac{1}{1-p_l}\right)\\
	&= p_k \left(1 + \sum_{k\neq l} p_l (1 - p_l)^{-1}  \right)
	\end{align*}
	\item \begin{align*}
	\pi_{kl} &= Pr(u_k \text{ in 1})Pr(u_l \text{ in 2}|u_k \text{ in 1}) + Pr(u_l \text{ in 1})Pr(u_k \text{ in 2}|u_l \text{ in 1})\\
	& = p_k\cdot \frac{x_l}{\sum_{j=1}^N x_j -x_k} + p_l\cdot \frac{x_k}{\sum_{j=1}^N x_j -x_l}\\
	&= p_k \frac{p_l}{1-p_k} + p_l \frac{p_k}{1-p_l}\\
	&= p_k p_l (\frac{1}{1-p_k}+\frac{1}{1-p_l})
	\end{align*}
\end{enumerate}
\end{Solution}
\begin{Solution}{{Aufgabe 16:}}
$N=320$, $n=4$, $p_1=1.2/80 = p_2$, $p_3=0.2/80$, $p_4 = 0.5/80$. Der Hansen-Hurwitz Schätzer für den Durchschnitt ist:
$$\hat{\bar{y}}_{HH}=\frac{1}{Nn}\sum_{k=1}^{N} \frac{y_k}{p_k}=3.021$$ Die geschätzte Varianz lautet:
$$\hat{V}(\hat{\bar{y}}_{HH}) = \frac{1}{N^2 \frac{1}{n(n-1)}}\sum_{k=1}^N\left(\frac{y_k}{p_k}-\frac{1}{n}\sum_{j=1}^N \frac{y_j}{p_j}\right)^2 = 2.326$$
\end{Solution}
\begin{Solution}{{Aufgabe 17:}}
Für Hansen-Hurwitz: $y_1/p_1 = 36.67$, $y_2/p_2 = 30$ und $y_3/p_3 = 50$. Für Horvitz-Thompson: $\pi_k = 1-(1-p_k)^m$, also $\pi_1 = 0.51$, $\pi_2=0.36$ und $\pi_3 = 0.75$. Damit dann $y_1/\pi_1 = 36.67$, $y_2/\pi_2 = 30$ und $y_3/\pi_3 = 50$. Zusammenfassend:
	\begin{center}
\begin{tabular}{|c|c|c|c|c|}
	\hline
	Stichprobe & Auswahl-Wkeit & Stipro-Werte & HH & HT \\
	\hline
	1,1 & 0.09 & (11,11) & 36,67 & 21.57 \\
	\hline
	2,2 & 0.04 & (6,6) & 30 & 16.67 \\
	\hline
	3,3 & 0.25 & (25,25) & 30 & 33.33 \\
	\hline
	1,2 & 0.06 & (11,6) & 33.33 & 38.24 \\
	\hline
	2,1 & 0.06 & (6,11) & 33.33 & 38.24 \\
	\hline
	1,3 & 0.15 & (11,25) & 43.33 & 54.90 \\
	\hline
	3,1 & 0.15 & (25,11) & 43.33 & 54.90 \\
	\hline
	2,3 & 0.10 & (6,25) & 40 & 50 \\
	\hline
	3,2 & 0.10 & (25,6) & 40 & 50 \\
	\hline
\end{tabular}
\end{center}
Der Mittelwert von HH ist 42 mit Std-Abweichung 5.89, während für HT der Mittelwert 42 mit Std-Abweichung 12.10 ist.
\end{Solution}
\begin{Solution}{{Aufgabe 18:}}
	\begin{align*}
	(N-1)S_{y_U}^2 &= \sum_{h=1}^H \sum_{k=1}^{N_h} (y_{h_k}-\bar{y}_U)^2 = \sum_{h=1}^H \sum_{k=1}^{N_h} \left[(y_{h_k}-\bar{y}_{U_h}) + (\bar{y}_{U_h}- \bar{y}_U)\right]^2\\
	&= \sum_{h=1}^H \sum_{k=1}^{N_h} (y_{h_k}-\bar{y}_{U_h})^2 + \sum_{h=1}^H \sum_{k=1}^{N_h} (\bar{y}_{U_h}- \bar{y}_U)^2 + 2\sum_{h=1}^H \sum_{k=1}^{N_h}(y_{h_k}-\bar{y}_{U_h})(\bar{y}_{U_h}- \bar{y}_U)
	\end{align*}
	Da $2\sum_{h=1}^H \sum_{k=1}^{N_h} (y_{h_k}-\bar{y}_{U_h})(\bar{y}_{U_h}- \bar{y}_U) = 2\sum_{h=1}^H (\bar{y}_{U_h}- \bar{y}_U) \underbrace{\sum_{k=1}^{N_h} (y_{h_k}-\bar{y}_{U_h})}_{=0} = 0$, folgt:
	\begin{align*}
	(N-1)S_{y_U}^2 &= \sum_{h=1}^H \sum_{k=1}^{N_h} (y_{h_k}-\bar{y}_{U_h})^2 + \sum_{h=1}^H \sum_{k=1}^{N_h} (\bar{y}_{U_h}- \bar{y}_U)^2\\
	 &= \sum_{h=1}^H (N_h-1)S_{y_{U_h}}^2 + \sum_{h=1}^H N_h (\bar{y}_{U_h}- \bar{y}_U)^2
	\end{align*}

%	Varianzvergleich für Mittelwertschätzer:\\
%	\begin{itemize}
%		\item Bei proportionaler Aufteilung: $n_h = W_h n$ und $n_h/N_h = n/N = f$. Also gilt für die Varianz:
%		\begin{align*}
%		V_{Prop}(\hat{\bar{y}}_\pi) =
%		\end{align*}
%		\item Einfache Zufallsstichprobe ohne Zurücklegen:
%		\begin{align*}
%		V_{SI}(\hat{\bar{y}}_\pi)=\frac{1-f}{n}S_{y_U}^2 = \frac{1-f}{n} \frac{1}{N-1} \left(\sum_{h=1}^H (N_h-1) S_{y_{U_h}}^2 + \sum_{h=1}^H N_h (\bar{y}_{U_h}-\bar{y}_U)^2\right)
%		\end{align*}
%	\end{itemize}
\end{Solution}
\begin{Solution}{{Aufgabe 19:}}
	\begin{enumerate}
\item Wahlprognose: $\hat{P} = \sum_{h=1}^3 W_h \hat{P}_h = 0.23$.\\
Durchschnittsalter: $\hat{\bar{y}}_\pi=\sum_{h=1}^H W_h \bar{y}_h=46.1$.\\
Da $n_h$ nicht gegeben ist, sind die Konfidenzintervalle nicht berechenbar.
\item Für die Optimale Aufteilung gilt $n_h = n \frac{N_h S_{y_{U_h}}}{\sum_{i=1}^N W_i S_{y_{U_i}}}$.\\
\begin{itemize}
\item Wahlprognose: Wir benötigen die Varianz der Personen, die die OP wählen, in den jeweiligen Schichten. Es gilt:
\begin{align*}
S_{y_{U_h}}^2 = \frac{1}{N_h-1}\sum_{U_h}(y_k - \bar{y}_{u_h})^2 = \left(\frac{1}{N_h-1}\sum y_k^2\right) -\bar{y}_{U_h}^2 = \left(\frac{N_h}{N_h-1} \frac{1}{N_h}\sum_{U_h} y_k^2\right) - \left(\frac{1}{N_h}\sum_{U_h}y_k\right)^2
\end{align*}
Da $y_k$ nur Werte 0 oder 1 annimmt, gilt $\frac{1}{N_h}\sum_{U_h} y_k^2=\frac{1}{N_h}\sum_{U_h} y_k = \hat{P}_h$. Außerdem ist $N_h/(N_h-1)\approx 1$. Folglich:
\begin{align*}
S_{y_{U_h}}^2 = \hat{P}_h - \hat{P}_h^2 = \hat{P}_h(1-\hat{P}_h)
\end{align*}
Somit errechnen wir: $n_1= n\cdot 0.2411$, $n_2= n\cdot 0.4393$ und $n_3= n\cdot 0.3169$.

\item Durchschnittsalter: Schätze $S_{y_{U_h}}$ durch $s_h$, dann ergibt sich:
$n_1 = n\cdot 0.2065$, $n_2 = n\cdot 0.5016$ und $n_3 = n\cdot 0.2920$.
\end{itemize}
\end{enumerate}
\end{Solution}
\begin{Solution}{{Aufgabe 20:}}
\begin{enumerate}
	\item Bei proportionaler Aufteilung gilt $n_h = n W_h$. Hier: $n=100$ und Gesamtanzahl an Bauernhöfen: 2010
	\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
		\hline
		Schicht & 1 & 2 & 3 & 4 & 5 & 6 & 7 & $\sum$ \\
		\hline
		$W_h$ & 0.196 & 0.229 & 0.195 & 0.166 & 0.084 & 0.056 & 0.074 & 1 \\
		\hline
		$n_h$ & 20 & 23 & 19 & 17 & 8 & 6 & 7 & 100 \\
		\hline
	\end{tabular}
	\end{center}
	\item Bei optimaler Aufteilung gilt $\frac{n_h}{n} = \frac{N_h s_h}{\sum_i N_i s_i}$.
\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		Schicht & 1 & 2 & 3 & 4 & 5 & 6 & 7\\
		\hline
		$n_h$ (ungerundet)	& 9.6 & 17.9 & 17.3 & 19.3 & 12.1 & 8.6 & 15.2\\
		\hline
		$n_h$ (gerundet) & 10 & 18 & 17 & 19 & 12 & 9 & 15\\
		\hline
	\end{tabular}
\end{center}
Genauigkeit:\\
Bei Schichtschätzung: $\hat{V}(\hat{\bar{y}}) = \sum_{h=1}^H \frac{N_h^2}{N^2}\frac{1-f_h}{n_h}S_{y_{s_h}}^2$. Somit
$\hat{V}_{prop}=3.2620$ und $\hat{V}_{opt} = 2.7254$.\\
Bei der einfachen Zufallsstichprobe gilt $\hat{V}_{EZ} = \frac{1}{n}\left(1-\frac{n}{N}\right) S_{y_s}^2$. Weiter gilt die Varianzzerlegungsformel $(N-1)S_{y_s}^2 = \sum_{h=1}^H (N_h-1)S_h^2 + \sum_{h=1}^H N_h (\bar{y}_{s_h}-\bar{y}_s)^2$. Nun ist $\bar{y}_s=\frac{1}{N}\sum_{h=1}^H N_h \bar{y}_h = 26.3168$ und $S_{y_s}^2 = 1243156/(N-1) = 618.7935$, dh. $\hat{V}_{EZ} = 5.880078$.\\
Effizienzvergleich:
\begin{align*}
\frac{\hat{V}_{EZ}}{\hat{V}_{prop}} = 1.8026 \qquad
\frac{\hat{V}_{EZ}}{\hat{V}_{opt}} = 2.1575
\end{align*}

\end{enumerate}
\end{Solution}
\begin{Solution}{{Aufgabe 21:}}
\begin{enumerate}
	\item $\frac{n_h}{n}=\frac{W_h s_h / \sqrt{c_h}}{\sum_i W_i s_i / \sqrt{c_i}}$, also: $n_1/n = 1/3$ und $n_2/n=2/3$.
	\item $n_1 = 1/3n$, $n_2=2/3 n$, vernachlässige $f_{1}$ und $f_2$:
	\begin{align*}
	V(\hat{\bar{y}}_U) &= \frac{1}{n_1}\frac{N_1}{N^2}(1-f_1)S_{y_1}^2 + \frac{1}{n_2}\frac{N_2}{N^2}(1-f_2)S_{y_2}^2\\
	&\approx \frac{1}{n_1}W_1^2 s_1^2 + \frac{1}{n_2}W_2^2 s_2^2 = \frac{1}{n}(3 W_1^2 S_1^2 + \frac{3}{2} W_2^2 S_2^2) \overset{!}{=} 1\\
	&\Leftrightarrow n =264, \text{ d.h. } n_1=264/3 = 88, n_2 = n-n_1 = 176
	\end{align*}
	\item Erhebungskosten: $C=88 \cdot 4 + 176 \cdot 9 = 1936$
\end{enumerate}

\end{Solution}
\begin{Solution}{{Aufgabe 22:}}
\begin{enumerate}
	\item Es gilt $n=400$, $W_1 = 0.62$ (männliche Fach-oder Hilfsarbeiter), $W_2=0.31 (weibliche Schreibkräfte)$ und $W_3 = 0.07$ (Angestellte mit leitenden Aufgaben). Vermutungen: $P_1 \in [0.4,0.45]$, $P_2 \in [0.2,0.3]$ und $P_3 \in [0.05,0.1]$. Deshalb Annahme:
	\begin{itemize}
		\item $P_1 = 0.425 \Rightarrow s_1^2 \approx P_1(1-P_1) = 0.2444$
		\item $P_2 = 0.25 \Rightarrow s_2^2 \approx P_2(1-P_2) = 0.1875$
		\item $P_3 = 0.075 \Rightarrow s_3^2 \approx P_3(1-P_3) = 0.0694$
	\end{itemize}
	Optimale Aufteilung: $n_1 = n \frac{W_1 s_1}{sum_{i=1}^3 W_i s_i}=267, n_2 = 117, n_3 = 16$
	\item $P_1=0.48$, $P_2=0.21$, $P_3=0.04$:
	\begin{align*}
	\sqrt{V(\hat{P})} &= \left(\sum_{h=1}^3W_h^2\frac{1}{n_h}\left(1-\frac{n_h}{N_h}\right)s_h^2\right)^{1/2}\\
	&= \left(\sum_{h=1}^3W_h^2\frac{1}{n_h}\left(1-\frac{n_h}{N_h}\right)\frac{N_h}{N_h-1}P_h(1-P_h)\right)^{1/2}\\
	&\approx \left(\sum_{h=1}^3W_h^2\frac{1}{n_h} P_h(1-P_h)\right)^{1/2}\\
	\end{align*}
	Mit $P_1 = 0.48 \Rightarrow s_1^2 \approx P_1(1-P_1)=0.2496$, $P_2 = 0.21 \Rightarrow s_2^2 \approx P_2(1-P_2)=0.1659$ und $P_3 = 0.04 \Rightarrow s_3^2 \approx P_3(1-P_3)=0.0384$. Daraus folgt, dass $n_1=276,n_2=112,n_3=12$. Somit $\sqrt{V(\hat{P})} = 0.02248$.
	\item $S^2 = \frac{1}{N-1}\sum_{h=1}^H(N_h-1)S_h^2 + \frac{1}{N-1}\sum_{h=1}^H N_h(\bar{y}_h - \bar{y}_U)^2 \approx \sum_{h=1}^H W_h S_h^2 + \sum_{h=1}^H W_h(P_h-P)^2$ mit $P=\sum_{h=1}^H W_h P_h=0.3655$. Die Varianz bei der einfachen Zufallsstichprobe ist ungefähr $V(\hat{P}) \approx 1/n S^2 = 0.02408^2$. Somit ist die gesuchte Standardabweichung 0.02408.

\end{enumerate}
\end{Solution}
\begin{Solution}{{Aufgabe 23:}}
$H=2, N_1=63, N_2 = 57, n_1 = 6, n_2 = 6$
\begin{enumerate}
	\item Körpergröße:\\ $\bar{y}_1 = 185, \bar{y}_2 = 169,67$. $\hat{\bar{y}}=\sum_{h=1}^{2}\frac{N_h}{N}\bar{y}_{s_h} = 177.72$\\
	Jeansträger:\\
	$\bar{y}_1=3/6$, $\bar{y}_2=2/6$, $\hat{P}=\frac{1}{120}(63\frac{3}{6}+57\frac{2}{6})=0.4208$
	\item Körpergröße:\\ $s_1^2 = 35.2$, $s_2^2 = 15.86$, d.h. $\hat{V}(\hat{\bar{y}}) = 6.225$\\
	Jeansträger:\\ $s_1^2=0.3, s_2^2=0.267$, d.h. $\hat{V}(\hat{\bar{y}}) = 0.0201$
	\item Körpergröße:\\ $\bar{y}=177.3$, $\hat{V}(\hat{\bar{y}})=6.55$\\
	Jeansträger: $\hat{P}=0.4167$, $\hat{V}(\hat{P})=0.01988$
\end{enumerate}
\end{Solution}
\begin{Solution}{{Aufgabe 24:}}
%Einstufige Clusterauswahl mit $K=1000$, $M=20$, $N=KM = 20000$, $k=10$, $n=kM=200$. Dann $\hat{P}=\frac{1}{Mk}\sum_{i=1}k y_i = \frac{1}{200}13 =0.065$. Mit

\end{Solution}
\begin{Solution}{{Aufgabe 25:}}
$N>80$ Millionen, Auswahlsatz vernachlässigbar. Definiere $$y_k = \begin{cases}
1 &\text{, falls } u_k \text{ die OP jetzt wählt}\\
0 &\text{, sonst}
\end{cases}$$ und $$x_k = \begin{cases}
1 &\text{, falls } u_k \text{ die OP früher gewählt hat}\\
0 &\text{, sonst}
\end{cases}$$
\begin{enumerate}
	\item Keine Vorinformation, also freie Schätzung: $\hat{P} = \frac{80}{200}=0.4$ und
	\begin{align*}
	\hat{V}(\hat{P}) = \frac{1}{n}(1-n/N)S_y^2 &\approx \frac{1}{n(n-1)}\left(\sum_{k=1}^n y_k^2 - n \bar{y}_s^2\right)\\
	&= \frac{1}{n(n-1)}\left(n \hat{P}-n\hat{P}^2\right) = \frac{1}{n-1}\hat{P}(1-\hat{P}) = 0.0012
	\end{align*}
\item Vorinformationen: In der Grundgesamtheit: $\bar{x}_U = 0.25$. In der Stichprobe: $\bar{x}_s = \frac{60}{200} = 0.3$
\begin{itemize}
	\item Differenzenschätzung: $\hat{P} = \bar{y}_s - \bar{x}_s + \bar{x}_U = 0.4-0.3+0.25 = 0.35$
	\begin{align*}
	\hat{V}(\hat{P}) \approx \frac{1}{n}(S_{y_s}^2+S_{x_s}^2-2S_{xy_s}) &= \frac{1}{n(n-1)}\sum_{k=1}^n\left(y_k - x_k - \bar{y}_s + \bar{x}_s\right)^2\\
	&= \frac{1}{n(n-1)}\left(\sum_{k=1}^n (y_k-x_k)^2 - n (\bar{y}_s+\bar{x}_s)^2\right)\\
	&= \frac{1}{n(n-1)}\left(20 - 200*20^2/200^2\right) = 0.00045
	\end{align*}
	da 20 mal $(y_k=1,x_k=0)$ während $(y_k=0,x_k=1)$ kein mal auftritt.
	\item Verhältnisschätzung: $\hat{P}=\bar{x}_U \frac{\bar{y}_s}{\bar{x}_s} = 0.25 \frac{0.4}{0.33}$ Approximierte Varianz ist
	\begin{align*}
	\hat{AV}(\hat{P}) = \frac{1}{n}(1-n/N)\left(S_{y_s}^2+\hat{r}S_{x_s}^2 - 2 \hat{r}S_{xy_s}\right) \approx \frac{1}{n}\left(S_{y_s}^2+\hat{r}S_{x_s}^2 - 2 \hat{r}S_{xy_s}\right)
	\end{align*}
	mit
	\begin{align*}
	\hat{r} &= 0.4/0.3\\
	S_{y_s}^2 &= \frac{n}{n-1}\hat{P}(1-\hat{P}) = 0.2412\\
	S_{x_s}^2 &= \frac{n}{n-1}\hat{P_x}(1-\hat{P_x}) = 200/199 \cdot 0.3 \cdot 0.7 = 0.2111\\
	S_{xy_s} &= \frac{1}{n-1}(\sum x_i y_i - n \bar{x}\bar{y}) = \frac{n}{n-1}(\bar{x}-\bar{x}\bar{y})=\frac{n}{n-1}\hat{P_x}(1-\hat{P}) = 200/199 \cdot 0.3 \cdot 0.6 = 0.1809
	\end{align*}
	Damit ist die approximierte Varianz gleich $0.00067$
\end{itemize}
\end{enumerate}
\end{Solution}
